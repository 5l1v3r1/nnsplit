{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(\"python_lib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm\n",
    "import nnsplit\n",
    "from nnsplit import train, utils, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = Path(\"cache\")\n",
    "cache_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## German"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f65995ecb0014b1aa685963ac7bc1cd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3000000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "paragraphs = train.xml_to_paragraphs(\"train_data/dewiki-20180920-corpus.xml\", max_n_paragraphs=3_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nnsplit.tokenizer.SoMaJoTokenizer(\"de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff8384c996ac41449c9d7c0779a268aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3000000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(cache_dir / \"de_data\" / \"tokenized_paragraphs.pkl\", \"wb\") as f:\n",
    "    for x in tokenizer.split(paragraphs, verbose=True):\n",
    "        f.write(pickle.dumps(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = train.xml_to_paragraphs(\"train_data/enwiki-20181001-corpus.xml\", max_n_paragraphs=3_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nnsplit.tokenizer.SoMaJoTokenizer(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(cache_dir / \"en_data\" / \"tokenized_paragraphs.pkl\", \"wb\") as f:\n",
    "    for x in tokenizer.split(paragraphs, verbose=True):\n",
    "        f.write(pickle.dumps(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model (german)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41f1ab8bc59d400988b909afefa989fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faulty paragraph:\n",
      "[[Token(text='.', whitespace='')], [Token(text='GLOBAL', whitespace=' '), Token(text='_set_float', whitespace=''), Token(text='Extend', whitespace=''), Token(text=';', whitespace=' '), Token(text=';', whitespace=' '), Token(text='Sprunglabel', whitespace=' '), Token(text='global', whitespace=' '), Token(text='sichtbar', whitespace=' '), Token(text='_set_float', whitespace=''), Token(text='Extend', whitespace=''), Token(text=':', whitespace=' '), Token(text=';', whitespace=' '), Token(text='Sprunglabel', whitespace=' '), Token(text='angeben', whitespace=''), Token(text=',', whitespace=' '), Token(text='das', whitespace=' '), Token(text='ist', whitespace=' '), Token(text='der', whitespace=' '), Token(text='Name', whitespace=' '), Token(text='des', whitespace=' '), Token(text='Unterprogramms', whitespace=''), Token(text=',', whitespace=' '), Token(text=';', whitespace=' '), Token(text='aus', whitespace=' '), Token(text='C', whitespace=' '), Token(text='ohne', whitespace=' '), Token(text='Unterstrich', whitespace=' '), Token(text='anzugeben', whitespace=''), Token(text='.', whitespace=' ')], [Token(text='I4', whitespace=' '), Token(text='=', whitespace=' '), Token(text='R4', whitespace=''), Token(text=';', whitespace=' '), Token(text=';', whitespace=' '), Token(text='Im', whitespace=' '), Token(text='Register', whitespace=' '), Token(text='R4', whitespace=' '), Token(text='wird', whitespace=' '), Token(text='der', whitespace=' '), Token(text='erste', whitespace=' '), Token(text='Parameter', whitespace=' '), Token(text='_float', whitespace=''), Token(text='Extend', whitespace=''), Token(text='*', whitespace=' '), Token(text='dst', whitespace=' '), Token(text='Ã¼bergeben', whitespace=''), Token(text='.', whitespace=' '), Token(text=';', whitespace=' '), Token(text='Da', whitespace=' '), Token(text='es', whitespace=' '), Token(text='eine', whitespace=' '), Token(text='Adresse', whitespace=' '), Token(text='ist', whitespace=''), Token(text=',', whitespace=' '), Token(text='wird', whitespace=' '), Token(text='diese', whitespace=' '), Token(text='in', whitespace=' '), Token(text='das', whitespace=' '), Token(text='Adressregister', whitespace=' '), Token(text='I4', whitespace=' '), Token(text='umgeladen', whitespace=''), Token(text='.', whitespace=' ')], [Token(text='PX', whitespace=' '), Token(text='=', whitespace=' '), Token(text='F8', whitespace=''), Token(text=';', whitespace=' '), Token(text=';', whitespace=' '), Token(text='Der', whitespace=' '), Token(text='zweite', whitespace=' '), Token(text='Parameter', whitespace=' '), Token(text='float', whitespace=' '), Token(text='nVal', whitespace=' '), Token(text='wird', whitespace=' '), Token(text='aus', whitespace=' '), Token(text='F8', whitespace=' '), Token(text='in', whitespace=' '), Token(text='das', whitespace=' '), Token(text='Register', whitespace=' '), Token(text='PX', whitespace=' '), Token(text='geladen', whitespace=''), Token(text='.', whitespace=' '), Token(text='dm', whitespace=''), Token(text='(', whitespace=''), Token(text='0', whitespace=''), Token(text=',', whitespace=''), Token(text='I4', whitespace=''), Token(text=')', whitespace=' '), Token(text='=', whitespace=' '), Token(text='PX1', whitespace=''), Token(text=';', whitespace=' '), Token(text=';', whitespace=' '), Token(text='Ein', whitespace=' '), Token(text='Teil', whitespace=' '), Token(text='des', whitespace=' '), Token(text='Inhaltes', whitespace=' '), Token(text='von', whitespace=' '), Token(text='PX', whitespace=''), Token(text=',', whitespace=' '), Token(text='in', whitespace=' '), Token(text='PX1', whitespace=' '), Token(text='sichtbar', whitespace=''), Token(text=',', whitespace=' '), Token(text='wird', whitespace=' '), Token(text='auf', whitespace=' '), Token(text=';', whitespace=' '), Token(text='der', whitespace=' '), Token(text='Adresse', whitespace=' '), Token(text='gespeichert', whitespace=''), Token(text=',', whitespace=' '), Token(text='die', whitespace=' '), Token(text='von', whitespace=' '), Token(text='I4', whitespace=' '), Token(text='gezeigert', whitespace=' '), Token(text='wird', whitespace=''), Token(text='.', whitespace=' '), Token(text='dm', whitespace=''), Token(text='(', whitespace=''), Token(text='1', whitespace=''), Token(text=',', whitespace=''), Token(text='I4', whitespace=''), Token(text=')', whitespace=' '), Token(text='=', whitespace=' '), Token(text='PX2', whitespace=''), Token(text=';', whitespace=' '), Token(text=';', whitespace=' '), Token(text='Speicherung', whitespace=' '), Token(text='des', whitespace=' '), Token(text='zweiten', whitespace=' '), Token(text='Teils', whitespace=' '), Token(text='auf', whitespace=' '), Token(text='der', whitespace=' '), Token(text='Folgeadresse', whitespace=' '), Token(text='!', whitespace=' ')], [Token(text='FUNCTION', whitespace=' '), Token(text='EPILOGUE', whitespace=''), Token(text=':', whitespace=' '), Token(text=';', whitespace=' '), Token(text='Standard-Abschluss', whitespace=' '), Token(text='des', whitespace=' '), Token(text='Unterprogramms', whitespace=''), Token(text=':', whitespace=' '), Token(text='i12', whitespace=' '), Token(text='=', whitespace=' '), Token(text='dm', whitespace=''), Token(text='(', whitespace=''), Token(text='-1', whitespace=''), Token(text=',', whitespace=''), Token(text='i6', whitespace=''), Token(text=')', whitespace=''), Token(text=';', whitespace=' '), Token(text=';', whitespace=' '), Token(text='Das', whitespace=' '), Token(text='Adressregister', whitespace=' '), Token(text='i12', whitespace=' '), Token(text='wird', whitespace=' '), Token(text='aus', whitespace=' '), Token(text='einer', whitespace=' '), Token(text='Adresse', whitespace=' '), Token(text='relativ', whitespace=' '), Token(text='zum', whitespace=' '), Token(text='Basepointer', whitespace=' '), Token(text=';(', whitespace=''), Token(text='hier', whitespace=' '), Token(text='i6', whitespace=''), Token(text=')', whitespace=' '), Token(text='geladen', whitespace=''), Token(text='.', whitespace=' ')], [Token(text='Das', whitespace=' '), Token(text='ist', whitespace=' '), Token(text='die', whitespace=' '), Token(text='RÃ¼cksprungadresse', whitespace=''), Token(text='.', whitespace=' '), Token(text='jump', whitespace=' '), Token(text='(', whitespace=''), Token(text='m14', whitespace=''), Token(text=',', whitespace=''), Token(text='i12', whitespace=''), Token(text=')', whitespace=' '), Token(text='(', whitespace=''), Token(text='DB', whitespace=''), Token(text=')', whitespace=' '), Token(text=';', whitespace=' '), Token(text='Das', whitespace=' '), Token(text='ist', whitespace=' '), Token(text='der', whitespace=' '), Token(text='RÃ¼cksprung', whitespace=' '), Token(text='unter', whitespace=' '), Token(text='Nutzung', whitespace=' '), Token(text='des', whitespace=' '), Token(text='Registers', whitespace=' '), Token(text='i12', whitespace=''), Token(text='.', whitespace=' ')], [Token(text='F0', whitespace=' '), Token(text='=', whitespace=' '), Token(text='F8', whitespace=''), Token(text=';', whitespace=' '), Token(text=';', whitespace=' '), Token(text='nach', whitespace=' '), Token(text='dem', whitespace=' '), Token(text='RÃ¼cksprung', whitespace=' '), Token(text='werden', whitespace=' '), Token(text='die', whitespace=' '), Token(text='noch', whitespace=' '), Token(text='im', whitespace=' '), Token(text='cashe', whitespace=' '), Token(text='stehenden', whitespace=' '), Token(text='Befehl', whitespace=' '), Token(text='verarbeitet', whitespace=''), Token(text=',', whitespace=' '), Token(text=';', whitespace=' '), Token(text='hier', whitespace=' '), Token(text='wird', whitespace=' '), Token(text='der', whitespace=' '), Token(text='Wert', whitespace=' '), Token(text='in', whitespace=' '), Token(text='F8', whitespace=' '), Token(text='nach', whitespace=' '), Token(text='dem', whitespace=' '), Token(text='Register', whitespace=' '), Token(text='R0', whitespace=' '), Token(text='geladen', whitespace=''), Token(text=',', whitespace=' '), Token(text='fÃ¼r', whitespace=' '), Token(text='return', whitespace=''), Token(text='.', whitespace=' ')], [Token(text='RFRAME', whitespace=''), Token(text=';', whitespace=' '), Token(text=';', whitespace=' '), Token(text='dieser', whitespace=' '), Token(text='Befehl', whitespace=' '), Token(text='korrigiert', whitespace=' '), Token(text='den', whitespace=' '), Token(text='Basepointer', whitespace=' '), Token(text='i6', whitespace=' '), Token(text='und', whitespace=' '), Token(text='Stackpointer', whitespace=' '), Token(text='i7', whitespace=''), Token(text='.', whitespace='')]]\n"
     ]
    }
   ],
   "source": [
    "sentences, labels = train.prepare_tokenized_paragraphs(cache_dir / \"de_data\" / \"tokenized_paragraphs.pkl\", \"de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(sentences, labels, test_size=0.1, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.031820</td>\n",
       "      <td>0.031392</td>\n",
       "      <td>02:04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "de_model = train.train(x_train, y_train, x_valid, y_valid, n_epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(de_model, Path(\"cache/de_data\") / \"model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bminixhofer/miniconda3/lib/python3.7/site-packages/tensorflowjs/converters/keras_h5_conversion.py:122: H5pyDeprecationWarning: The default file mode will change to 'r' (read-only) in h5py 3.0. To suppress this warning, pass the mode you need to h5py.File(), or set the global default h5.get_config().default_file_mode, or set the environment variable H5PY_DEFAULT_READONLY=1. Available modes are: 'r', 'r+', 'w', 'w-'/'x', 'a'. See the docs for details.\n",
      "  return h5py.File(h5file)\n"
     ]
    }
   ],
   "source": [
    "utils.store_model(de_model, \"data/de\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_model = torch.load(Path(\"cache/de_data\") / \"model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2f6ff82260345e197c6ac1e807462fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1172.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Target: Tokenize \n",
      "\n",
      "F1: 0.9976900240406937\n",
      "Precision: 0.997930734517165\n",
      "Recall: 0.9974494296595764\n",
      "\n",
      "\n",
      "\n",
      "Target: Sentencize \n",
      "\n",
      "F1: 0.9585695555203936\n",
      "Precision: 0.937686412832653\n",
      "Recall: 0.9804040596674962\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.evaluate(de_model.cuda().half(), x_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04f6eff8d54045e787eb1734644db616",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=977.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Target: Tokenize \n",
      "\n",
      "F1: 0.9985709185426798\n",
      "Precision: 0.9977352694599613\n",
      "Recall: 0.9994079685876922\n",
      "\n",
      "\n",
      "\n",
      "Target: Sentencize \n",
      "\n",
      "F1: 0.9581387972891665\n",
      "Precision: 0.9362705478411474\n",
      "Recall: 0.9810530203414978\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.evaluate(de_model.cuda().half(), x_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model = torch.quantization.quantize_dynamic(de_model.float().cpu(), {nn.LSTM, nn.Linear}, dtype=torch.qint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e110baec60f442c5afd934a8d22af664",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=977.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Target: Tokenize \n",
      "\n",
      "F1: 0.9985806618134485\n",
      "Precision: 0.9977847594261771\n",
      "Recall: 0.9993778349483429\n",
      "\n",
      "\n",
      "\n",
      "Target: Sentencize \n",
      "\n",
      "F1: 0.9581120292762628\n",
      "Precision: 0.9363237215520647\n",
      "Recall: 0.9809385261100492\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.evaluate(quantized_model, x_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model (english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7e46138c5bf469a8adbb36216a1c685",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faulty paragraph:\n",
      "[[Token(text='.', whitespace='')], [Token(text='Info', whitespace=' '), Token(text='in', whitespace=' '), Token(text='1987', whitespace=' '), Token(text='gave', whitespace=' '), Token(text='the', whitespace=' '), Token(text='Commodore', whitespace=' '), Token(text='64', whitespace=' '), Token(text='version', whitespace=' '), Token(text='four', whitespace=' '), Token(text='stars', whitespace=' '), Token(text='out', whitespace=' '), Token(text='of', whitespace=' '), Token(text='five', whitespace=''), Token(text=',', whitespace=' '), Token(text='describing', whitespace=' '), Token(text='it', whitespace=' '), Token(text='as', whitespace=' '), Token(text='\"', whitespace=''), Token(text='fun', whitespace=' '), Token(text='to', whitespace=' '), Token(text='play', whitespace=''), Token(text=',', whitespace=' '), Token(text='though', whitespace=' '), Token(text='Infocom', whitespace=' '), Token(text='has', whitespace=' '), Token(text='produced', whitespace=' '), Token(text='more', whitespace=' '), Token(text='challenging', whitespace=' '), Token(text='standard', whitespace=''), Token(text='-', whitespace=''), Token(text='level', whitespace=' '), Token(text='text', whitespace=' '), Token(text='adventures', whitespace=' '), Token(text='...', whitespace=' '), Token(text='a', whitespace=' '), Token(text='lot', whitespace=' '), Token(text='of', whitespace=' '), Token(text='giggles', whitespace=' '), Token(text='in', whitespace=' '), Token(text='this', whitespace=' '), Token(text='one', whitespace=''), Token(text='\"', whitespace=''), Token(text='.', whitespace=' ')], [Token(text='The', whitespace=' '), Token(text='magazine', whitespace=' '), Token(text='assured', whitespace=' '), Token(text='readers', whitespace=' '), Token(text='they', whitespace=' '), Token(text='would', whitespace=''), Token(text=\"n't\", whitespace=' '), Token(text='be', whitespace=' '), Token(text='offended', whitespace=' '), Token(text='by', whitespace=' '), Token(text='the', whitespace=' '), Token(text='game', whitespace=''), Token(text=',', whitespace=' '), Token(text='as', whitespace=' '), Token(text='even', whitespace=' '), Token(text='its', whitespace=' '), Token(text='lewdest', whitespace=' '), Token(text='\"', whitespace=''), Token(text='naughtiness', whitespace=''), Token(text='\"', whitespace=' '), Token(text='level', whitespace=' '), Token(text='is', whitespace=' '), Token(text='relatively', whitespace=' '), Token(text='tame', whitespace=''), Token(text='.', whitespace=' ')], [Token(text='In', whitespace=' '), Token(text='1988', whitespace=''), Token(text=',', whitespace=' '), Token(text='Tom', whitespace=' '), Token(text='Clancy', whitespace=' '), Token(text='named', whitespace=' '), Token(text='Leather', whitespace=' '), Token(text='Goddesses', whitespace=' '), Token(text='of', whitespace=' '), Token(text='Phobos', whitespace=' '), Token(text='one', whitespace=' '), Token(text='of', whitespace=' '), Token(text='his', whitespace=' '), Token(text='favorite', whitespace=' '), Token(text='computer', whitespace=' '), Token(text='games', whitespace=''), Token(text=',', whitespace=' '), Token(text='stating', whitespace=' '), Token(text='\"', whitespace=''), Token(text='I', whitespace=''), Token(text=\"'d\", whitespace=' '), Token(text='like', whitespace=' '), Token(text='to', whitespace=' '), Token(text='meet', whitespace=' '), Token(text='whoever', whitespace=' '), Token(text='wrote', whitespace=' '), Token(text='that', whitespace=''), Token(text='.', whitespace=' ')], [Token(text='I', whitespace=' '), Token(text='just', whitespace=' '), Token(text='do', whitespace=''), Token(text=\"n't\", whitespace=' '), Token(text='know', whitespace=' '), Token(text='what', whitespace=' '), Token(text='asylum', whitespace=' '), Token(text='to', whitespace=' '), Token(text='go', whitespace=' '), Token(text='to', whitespace=''), Token(text='\"', whitespace=''), Token(text='.', whitespace='')]]\n"
     ]
    }
   ],
   "source": [
    "sentences, labels = train.prepare_tokenized_paragraphs(cache_dir / \"en_data\" / \"tokenized_paragraphs.pkl\", \"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(sentences, labels, test_size=0.1, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.040443</td>\n",
       "      <td>0.038892</td>\n",
       "      <td>20:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.032624</td>\n",
       "      <td>0.031846</td>\n",
       "      <td>20:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.030703</td>\n",
       "      <td>0.030959</td>\n",
       "      <td>20:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.030231</td>\n",
       "      <td>0.030827</td>\n",
       "      <td>20:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.029231</td>\n",
       "      <td>0.030218</td>\n",
       "      <td>20:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.028824</td>\n",
       "      <td>0.029291</td>\n",
       "      <td>20:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.029228</td>\n",
       "      <td>0.028684</td>\n",
       "      <td>20:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.028699</td>\n",
       "      <td>0.027927</td>\n",
       "      <td>20:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.026893</td>\n",
       "      <td>0.027063</td>\n",
       "      <td>20:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.027493</td>\n",
       "      <td>0.026111</td>\n",
       "      <td>20:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.024701</td>\n",
       "      <td>0.025307</td>\n",
       "      <td>20:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.024740</td>\n",
       "      <td>0.024440</td>\n",
       "      <td>20:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.023115</td>\n",
       "      <td>0.023700</td>\n",
       "      <td>20:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.022261</td>\n",
       "      <td>0.023100</td>\n",
       "      <td>20:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.022435</td>\n",
       "      <td>0.022958</td>\n",
       "      <td>20:05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "en_model = train.train(x_train, y_train, x_valid, y_valid, n_epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(en_model, Path(\"cache/en_data\") / \"model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bminixhofer/miniconda3/lib/python3.7/site-packages/tensorflowjs/converters/keras_h5_conversion.py:122: H5pyDeprecationWarning: The default file mode will change to 'r' (read-only) in h5py 3.0. To suppress this warning, pass the mode you need to h5py.File(), or set the global default h5.get_config().default_file_mode, or set the environment variable H5PY_DEFAULT_READONLY=1. Available modes are: 'r', 'r+', 'w', 'w-'/'x', 'a'. See the docs for details.\n",
      "  return h5py.File(h5file)\n"
     ]
    }
   ],
   "source": [
    "utils.store_model(en_model, \"data/en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnsplit import NNSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = NNSplit(utils.load_model(\"data/de\").float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[Token(text='Das', whitespace=' '),\n",
       "   Token(text='ist', whitespace=' '),\n",
       "   Token(text='ein', whitespace=' '),\n",
       "   Token(text='Test', whitespace=' ')],\n",
       "  [Token(text='Das', whitespace=' '),\n",
       "   Token(text='ist', whitespace=' '),\n",
       "   Token(text='noch', whitespace=' '),\n",
       "   Token(text='ein', whitespace=' '),\n",
       "   Token(text='Test', whitespace=''),\n",
       "   Token(text='.', whitespace='')]]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitter.split([\"Das ist ein Test Das ist noch ein Test.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = NNSplit(utils.load_model(\"data/en\").float())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1_000_000\n",
    "x = torch.load(cache_dir / \"de_data/all_sentences.pt\")[:n]\n",
    "y = torch.load(cache_dir / \"de_data/all_labels.pt\")[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(x, y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.035032</td>\n",
       "      <td>0.035142</td>\n",
       "      <td>01:51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = train.train(x_train, y_train, x_valid, y_valid, n_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.002735</td>\n",
       "      <td>0.002637</td>\n",
       "      <td>01:54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = train.train(x_train, y_train, x_valid, y_valid, n_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model = torch.quantization.quantize_dynamic(model.float().cpu(), {nn.LSTM, nn.Linear}, dtype=torch.qint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/98 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|          | 1/98 [00:00<01:09,  1.41it/s]\u001b[A\n",
      "  2%|â         | 2/98 [00:01<01:09,  1.39it/s]\u001b[A\n",
      "  3%|â         | 3/98 [00:02<01:12,  1.32it/s]\u001b[A\n",
      "  4%|â         | 4/98 [00:03<01:22,  1.14it/s]\u001b[A\n",
      "  5%|â         | 5/98 [00:04<01:28,  1.06it/s]\u001b[A\n",
      "  6%|â         | 6/98 [00:05<01:19,  1.16it/s]\u001b[A\n",
      "  7%|â         | 7/98 [00:06<01:28,  1.03it/s]\u001b[A\n",
      "  8%|â         | 8/98 [00:07<01:27,  1.03it/s]\u001b[A\n",
      "  9%|â         | 9/98 [00:08<01:25,  1.04it/s]\u001b[A\n",
      " 10%|â         | 10/98 [00:09<01:27,  1.01it/s]\u001b[A\n",
      " 11%|â         | 11/98 [00:10<01:24,  1.03it/s]\u001b[A\n",
      " 12%|ââ        | 12/98 [00:11<01:19,  1.09it/s]\u001b[A\n",
      " 13%|ââ        | 13/98 [00:12<01:17,  1.10it/s]\u001b[A\n",
      " 14%|ââ        | 14/98 [00:13<01:23,  1.01it/s]\u001b[A\n",
      " 15%|ââ        | 15/98 [00:14<01:23,  1.00s/it]\u001b[A\n",
      " 16%|ââ        | 16/98 [00:15<01:28,  1.08s/it]\u001b[A\n",
      " 17%|ââ        | 17/98 [00:16<01:23,  1.03s/it]\u001b[A\n",
      " 18%|ââ        | 18/98 [00:17<01:23,  1.04s/it]\u001b[A\n",
      " 19%|ââ        | 19/98 [00:18<01:25,  1.08s/it]\u001b[A\n",
      " 20%|ââ        | 20/98 [00:19<01:23,  1.07s/it]\u001b[A\n",
      " 21%|âââ       | 21/98 [00:20<01:16,  1.01it/s]\u001b[A\n",
      " 22%|âââ       | 22/98 [00:21<01:18,  1.03s/it]\u001b[A\n",
      " 23%|âââ       | 23/98 [00:22<01:11,  1.05it/s]\u001b[A\n",
      " 24%|âââ       | 24/98 [00:23<01:13,  1.01it/s]\u001b[A\n",
      " 26%|âââ       | 25/98 [00:24<01:06,  1.10it/s]\u001b[A\n",
      " 27%|âââ       | 26/98 [00:25<01:04,  1.11it/s]\u001b[A\n",
      " 28%|âââ       | 27/98 [00:26<01:05,  1.08it/s]\u001b[A\n",
      " 29%|âââ       | 28/98 [00:26<01:02,  1.12it/s]\u001b[A\n",
      " 30%|âââ       | 29/98 [00:27<01:03,  1.09it/s]\u001b[A\n",
      " 31%|âââ       | 30/98 [00:28<01:04,  1.05it/s]\u001b[A\n",
      " 32%|ââââ      | 31/98 [00:29<01:06,  1.00it/s]\u001b[A\n",
      " 33%|ââââ      | 32/98 [00:30<01:00,  1.09it/s]\u001b[A\n",
      " 34%|ââââ      | 33/98 [00:31<01:04,  1.01it/s]\u001b[A\n",
      " 35%|ââââ      | 34/98 [00:33<01:12,  1.13s/it]\u001b[A\n",
      " 36%|ââââ      | 35/98 [00:34<01:11,  1.14s/it]\u001b[A\n",
      " 37%|ââââ      | 36/98 [00:35<01:05,  1.06s/it]\u001b[A\n",
      " 38%|ââââ      | 37/98 [00:36<00:58,  1.05it/s]\u001b[A\n",
      " 39%|ââââ      | 38/98 [00:36<00:54,  1.11it/s]\u001b[A\n",
      " 40%|ââââ      | 39/98 [00:37<00:48,  1.21it/s]\u001b[A\n",
      " 41%|ââââ      | 40/98 [00:38<00:49,  1.18it/s]\u001b[A\n",
      " 42%|âââââ     | 41/98 [00:39<00:44,  1.28it/s]\u001b[A\n",
      " 43%|âââââ     | 42/98 [00:39<00:43,  1.29it/s]\u001b[A\n",
      " 44%|âââââ     | 43/98 [00:40<00:45,  1.21it/s]\u001b[A\n",
      " 45%|âââââ     | 44/98 [00:41<00:43,  1.25it/s]\u001b[A\n",
      " 46%|âââââ     | 45/98 [00:42<00:41,  1.27it/s]\u001b[A\n",
      " 47%|âââââ     | 46/98 [00:43<00:41,  1.24it/s]\u001b[A\n",
      " 48%|âââââ     | 47/98 [00:43<00:40,  1.25it/s]\u001b[A\n",
      " 49%|âââââ     | 48/98 [00:44<00:39,  1.28it/s]\u001b[A\n",
      " 50%|âââââ     | 49/98 [00:45<00:38,  1.26it/s]\u001b[A\n",
      " 51%|âââââ     | 50/98 [00:46<00:39,  1.22it/s]\u001b[A\n",
      " 52%|ââââââ    | 51/98 [00:46<00:35,  1.31it/s]\u001b[A\n",
      " 53%|ââââââ    | 52/98 [00:48<00:49,  1.07s/it]\u001b[A\n",
      " 54%|ââââââ    | 53/98 [00:49<00:45,  1.01s/it]\u001b[A\n",
      " 55%|ââââââ    | 54/98 [00:50<00:43,  1.00it/s]\u001b[A\n",
      " 56%|ââââââ    | 55/98 [00:51<00:40,  1.06it/s]\u001b[A\n",
      " 57%|ââââââ    | 56/98 [00:52<00:36,  1.14it/s]\u001b[A\n",
      " 58%|ââââââ    | 57/98 [00:52<00:33,  1.22it/s]\u001b[A\n",
      " 59%|ââââââ    | 58/98 [00:53<00:32,  1.23it/s]\u001b[A\n",
      " 60%|ââââââ    | 59/98 [00:54<00:32,  1.20it/s]\u001b[A\n",
      " 61%|ââââââ    | 60/98 [00:55<00:31,  1.20it/s]\u001b[A\n",
      " 62%|âââââââ   | 61/98 [00:55<00:29,  1.26it/s]\u001b[A\n",
      " 63%|âââââââ   | 62/98 [00:57<00:31,  1.14it/s]\u001b[A\n",
      " 64%|âââââââ   | 63/98 [00:58<00:31,  1.11it/s]\u001b[A\n",
      " 65%|âââââââ   | 64/98 [00:58<00:28,  1.19it/s]\u001b[A\n",
      " 66%|âââââââ   | 65/98 [00:59<00:26,  1.24it/s]\u001b[A\n",
      " 67%|âââââââ   | 66/98 [01:00<00:25,  1.27it/s]\u001b[A\n",
      " 68%|âââââââ   | 67/98 [01:00<00:23,  1.33it/s]\u001b[A\n",
      " 69%|âââââââ   | 68/98 [01:01<00:21,  1.38it/s]\u001b[A\n",
      " 70%|âââââââ   | 69/98 [01:02<00:21,  1.36it/s]\u001b[A\n",
      " 71%|ââââââââ  | 70/98 [01:03<00:21,  1.30it/s]\u001b[A\n",
      " 72%|ââââââââ  | 71/98 [01:03<00:20,  1.29it/s]\u001b[A\n",
      " 73%|ââââââââ  | 72/98 [01:04<00:20,  1.29it/s]\u001b[A\n",
      " 74%|ââââââââ  | 73/98 [01:05<00:19,  1.28it/s]\u001b[A\n",
      " 76%|ââââââââ  | 74/98 [01:06<00:18,  1.32it/s]\u001b[A\n",
      " 77%|ââââââââ  | 75/98 [01:06<00:17,  1.32it/s]\u001b[A\n",
      " 78%|ââââââââ  | 76/98 [01:07<00:16,  1.34it/s]\u001b[A\n",
      " 79%|ââââââââ  | 77/98 [01:08<00:15,  1.39it/s]\u001b[A\n",
      " 80%|ââââââââ  | 78/98 [01:09<00:14,  1.36it/s]\u001b[A\n",
      " 81%|ââââââââ  | 79/98 [01:09<00:14,  1.31it/s]\u001b[A\n",
      " 82%|âââââââââ | 80/98 [01:10<00:13,  1.35it/s]\u001b[A\n",
      " 83%|âââââââââ | 81/98 [01:11<00:12,  1.31it/s]\u001b[A\n",
      " 84%|âââââââââ | 82/98 [01:12<00:13,  1.18it/s]\u001b[A\n",
      " 85%|âââââââââ | 83/98 [01:13<00:12,  1.16it/s]\u001b[A\n",
      " 86%|âââââââââ | 84/98 [01:14<00:12,  1.13it/s]\u001b[A\n",
      " 87%|âââââââââ | 85/98 [01:15<00:11,  1.16it/s]\u001b[A\n",
      " 88%|âââââââââ | 86/98 [01:15<00:09,  1.20it/s]\u001b[A\n",
      " 89%|âââââââââ | 87/98 [01:16<00:08,  1.24it/s]\u001b[A\n",
      " 90%|âââââââââ | 88/98 [01:17<00:07,  1.30it/s]\u001b[A\n",
      " 91%|âââââââââ | 89/98 [01:18<00:07,  1.27it/s]\u001b[A\n",
      " 92%|ââââââââââ| 90/98 [01:18<00:06,  1.30it/s]\u001b[A\n",
      " 93%|ââââââââââ| 91/98 [01:19<00:05,  1.22it/s]\u001b[A\n",
      " 94%|ââââââââââ| 92/98 [01:20<00:04,  1.27it/s]\u001b[A\n",
      " 95%|ââââââââââ| 93/98 [01:21<00:04,  1.22it/s]\u001b[A\n",
      " 96%|ââââââââââ| 94/98 [01:22<00:03,  1.14it/s]\u001b[A\n",
      " 97%|ââââââââââ| 95/98 [01:23<00:02,  1.18it/s]\u001b[A\n",
      " 98%|ââââââââââ| 96/98 [01:23<00:01,  1.20it/s]\u001b[A\n",
      " 99%|ââââââââââ| 97/98 [01:24<00:00,  1.27it/s]\u001b[A\n",
      "100%|ââââââââââ| 98/98 [01:25<00:00,  1.15it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: Tokenize \n",
      "\n",
      "F1: 0.9982945732838129\n",
      "Precision: 0.9974475067007087\n",
      "Recall: 0.9991430798056675\n",
      "\n",
      "\n",
      "\n",
      "Target: Sentencize \n",
      "\n",
      "F1: 0.897711766558203\n",
      "Precision: 0.8391659852820932\n",
      "Recall: 0.9650393049234588\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.evaluate(quantized_model, x_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/98 [00:00<?, ?it/s]\u001b[A\n",
      " 11%|â         | 11/98 [00:00<00:00, 109.64it/s]\u001b[A\n",
      " 22%|âââ       | 22/98 [00:00<00:00, 107.66it/s]\u001b[A\n",
      " 35%|ââââ      | 34/98 [00:00<00:00, 109.46it/s]\u001b[A\n",
      " 49%|âââââ     | 48/98 [00:00<00:00, 116.24it/s]\u001b[A\n",
      " 63%|âââââââ   | 62/98 [00:00<00:00, 120.74it/s]\u001b[A\n",
      " 77%|ââââââââ  | 75/98 [00:00<00:00, 121.12it/s]\u001b[A\n",
      "100%|ââââââââââ| 98/98 [00:00<00:00, 122.83it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: Tokenize \n",
      "\n",
      "F1: 0.9973287289215611\n",
      "Precision: 0.9961537390852487\n",
      "Recall: 0.9985064938947623\n",
      "\n",
      "\n",
      "\n",
      "Target: Sentencize \n",
      "\n",
      "F1: 0.9055118110236221\n",
      "Precision: 0.8516719990905105\n",
      "Recall: 0.9666181268548045\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.evaluate(model.cuda().half(), x_valid, y_valid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
