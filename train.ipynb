{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(\"python_lib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from tqdm.auto import tqdm\n",
    "import nnsplit\n",
    "from nnsplit import train, utils, models, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = Path(\"cache\")\n",
    "cache_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## German"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = train.xml_to_paragraphs(\"train_data/dewiki-20180920-corpus.xml\", max_n_paragraphs=3_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nnsplit.tokenizer.SoMaJoTokenizer(\"de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(cache_dir / \"de_data\" / \"tokenized_paragraphs.pkl\", \"wb\") as f:\n",
    "    for x in tokenizer.split(paragraphs, verbose=True):\n",
    "        f.write(pickle.dumps(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = train.xml_to_paragraphs(\"train_data/enwiki-20181001-corpus.xml\", max_n_paragraphs=3_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nnsplit.tokenizer.SoMaJoTokenizer(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(cache_dir / \"en_data\" / \"tokenized_paragraphs.pkl\", \"wb\") as f:\n",
    "    for x in tokenizer.split(paragraphs, verbose=True):\n",
    "        f.write(pickle.dumps(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model (german)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f80399d04db9447288b58a1439e44039",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faulty paragraph:\n",
      "[[Token(text='.', whitespace='')], [Token(text='GLOBAL', whitespace=' '), Token(text='_set_float', whitespace=''), Token(text='Extend', whitespace=''), Token(text=';', whitespace=' '), Token(text=';', whitespace=' '), Token(text='Sprunglabel', whitespace=' '), Token(text='global', whitespace=' '), Token(text='sichtbar', whitespace=' '), Token(text='_set_float', whitespace=''), Token(text='Extend', whitespace=''), Token(text=':', whitespace=' '), Token(text=';', whitespace=' '), Token(text='Sprunglabel', whitespace=' '), Token(text='angeben', whitespace=''), Token(text=',', whitespace=' '), Token(text='das', whitespace=' '), Token(text='ist', whitespace=' '), Token(text='der', whitespace=' '), Token(text='Name', whitespace=' '), Token(text='des', whitespace=' '), Token(text='Unterprogramms', whitespace=''), Token(text=',', whitespace=' '), Token(text=';', whitespace=' '), Token(text='aus', whitespace=' '), Token(text='C', whitespace=' '), Token(text='ohne', whitespace=' '), Token(text='Unterstrich', whitespace=' '), Token(text='anzugeben', whitespace=''), Token(text='.', whitespace=' ')], [Token(text='I4', whitespace=' '), Token(text='=', whitespace=' '), Token(text='R4', whitespace=''), Token(text=';', whitespace=' '), Token(text=';', whitespace=' '), Token(text='Im', whitespace=' '), Token(text='Register', whitespace=' '), Token(text='R4', whitespace=' '), Token(text='wird', whitespace=' '), Token(text='der', whitespace=' '), Token(text='erste', whitespace=' '), Token(text='Parameter', whitespace=' '), Token(text='_float', whitespace=''), Token(text='Extend', whitespace=''), Token(text='*', whitespace=' '), Token(text='dst', whitespace=' '), Token(text='übergeben', whitespace=''), Token(text='.', whitespace=' '), Token(text=';', whitespace=' '), Token(text='Da', whitespace=' '), Token(text='es', whitespace=' '), Token(text='eine', whitespace=' '), Token(text='Adresse', whitespace=' '), Token(text='ist', whitespace=''), Token(text=',', whitespace=' '), Token(text='wird', whitespace=' '), Token(text='diese', whitespace=' '), Token(text='in', whitespace=' '), Token(text='das', whitespace=' '), Token(text='Adressregister', whitespace=' '), Token(text='I4', whitespace=' '), Token(text='umgeladen', whitespace=''), Token(text='.', whitespace=' ')], [Token(text='PX', whitespace=' '), Token(text='=', whitespace=' '), Token(text='F8', whitespace=''), Token(text=';', whitespace=' '), Token(text=';', whitespace=' '), Token(text='Der', whitespace=' '), Token(text='zweite', whitespace=' '), Token(text='Parameter', whitespace=' '), Token(text='float', whitespace=' '), Token(text='nVal', whitespace=' '), Token(text='wird', whitespace=' '), Token(text='aus', whitespace=' '), Token(text='F8', whitespace=' '), Token(text='in', whitespace=' '), Token(text='das', whitespace=' '), Token(text='Register', whitespace=' '), Token(text='PX', whitespace=' '), Token(text='geladen', whitespace=''), Token(text='.', whitespace=' '), Token(text='dm', whitespace=''), Token(text='(', whitespace=''), Token(text='0', whitespace=''), Token(text=',', whitespace=''), Token(text='I4', whitespace=''), Token(text=')', whitespace=' '), Token(text='=', whitespace=' '), Token(text='PX1', whitespace=''), Token(text=';', whitespace=' '), Token(text=';', whitespace=' '), Token(text='Ein', whitespace=' '), Token(text='Teil', whitespace=' '), Token(text='des', whitespace=' '), Token(text='Inhaltes', whitespace=' '), Token(text='von', whitespace=' '), Token(text='PX', whitespace=''), Token(text=',', whitespace=' '), Token(text='in', whitespace=' '), Token(text='PX1', whitespace=' '), Token(text='sichtbar', whitespace=''), Token(text=',', whitespace=' '), Token(text='wird', whitespace=' '), Token(text='auf', whitespace=' '), Token(text=';', whitespace=' '), Token(text='der', whitespace=' '), Token(text='Adresse', whitespace=' '), Token(text='gespeichert', whitespace=''), Token(text=',', whitespace=' '), Token(text='die', whitespace=' '), Token(text='von', whitespace=' '), Token(text='I4', whitespace=' '), Token(text='gezeigert', whitespace=' '), Token(text='wird', whitespace=''), Token(text='.', whitespace=' '), Token(text='dm', whitespace=''), Token(text='(', whitespace=''), Token(text='1', whitespace=''), Token(text=',', whitespace=''), Token(text='I4', whitespace=''), Token(text=')', whitespace=' '), Token(text='=', whitespace=' '), Token(text='PX2', whitespace=''), Token(text=';', whitespace=' '), Token(text=';', whitespace=' '), Token(text='Speicherung', whitespace=' '), Token(text='des', whitespace=' '), Token(text='zweiten', whitespace=' '), Token(text='Teils', whitespace=' '), Token(text='auf', whitespace=' '), Token(text='der', whitespace=' '), Token(text='Folgeadresse', whitespace=' '), Token(text='!', whitespace=' ')], [Token(text='FUNCTION', whitespace=' '), Token(text='EPILOGUE', whitespace=''), Token(text=':', whitespace=' '), Token(text=';', whitespace=' '), Token(text='Standard-Abschluss', whitespace=' '), Token(text='des', whitespace=' '), Token(text='Unterprogramms', whitespace=''), Token(text=':', whitespace=' '), Token(text='i12', whitespace=' '), Token(text='=', whitespace=' '), Token(text='dm', whitespace=''), Token(text='(', whitespace=''), Token(text='-1', whitespace=''), Token(text=',', whitespace=''), Token(text='i6', whitespace=''), Token(text=')', whitespace=''), Token(text=';', whitespace=' '), Token(text=';', whitespace=' '), Token(text='Das', whitespace=' '), Token(text='Adressregister', whitespace=' '), Token(text='i12', whitespace=' '), Token(text='wird', whitespace=' '), Token(text='aus', whitespace=' '), Token(text='einer', whitespace=' '), Token(text='Adresse', whitespace=' '), Token(text='relativ', whitespace=' '), Token(text='zum', whitespace=' '), Token(text='Basepointer', whitespace=' '), Token(text=';(', whitespace=''), Token(text='hier', whitespace=' '), Token(text='i6', whitespace=''), Token(text=')', whitespace=' '), Token(text='geladen', whitespace=''), Token(text='.', whitespace=' ')], [Token(text='Das', whitespace=' '), Token(text='ist', whitespace=' '), Token(text='die', whitespace=' '), Token(text='Rücksprungadresse', whitespace=''), Token(text='.', whitespace=' '), Token(text='jump', whitespace=' '), Token(text='(', whitespace=''), Token(text='m14', whitespace=''), Token(text=',', whitespace=''), Token(text='i12', whitespace=''), Token(text=')', whitespace=' '), Token(text='(', whitespace=''), Token(text='DB', whitespace=''), Token(text=')', whitespace=' '), Token(text=';', whitespace=' '), Token(text='Das', whitespace=' '), Token(text='ist', whitespace=' '), Token(text='der', whitespace=' '), Token(text='Rücksprung', whitespace=' '), Token(text='unter', whitespace=' '), Token(text='Nutzung', whitespace=' '), Token(text='des', whitespace=' '), Token(text='Registers', whitespace=' '), Token(text='i12', whitespace=''), Token(text='.', whitespace=' ')], [Token(text='F0', whitespace=' '), Token(text='=', whitespace=' '), Token(text='F8', whitespace=''), Token(text=';', whitespace=' '), Token(text=';', whitespace=' '), Token(text='nach', whitespace=' '), Token(text='dem', whitespace=' '), Token(text='Rücksprung', whitespace=' '), Token(text='werden', whitespace=' '), Token(text='die', whitespace=' '), Token(text='noch', whitespace=' '), Token(text='im', whitespace=' '), Token(text='cashe', whitespace=' '), Token(text='stehenden', whitespace=' '), Token(text='Befehl', whitespace=' '), Token(text='verarbeitet', whitespace=''), Token(text=',', whitespace=' '), Token(text=';', whitespace=' '), Token(text='hier', whitespace=' '), Token(text='wird', whitespace=' '), Token(text='der', whitespace=' '), Token(text='Wert', whitespace=' '), Token(text='in', whitespace=' '), Token(text='F8', whitespace=' '), Token(text='nach', whitespace=' '), Token(text='dem', whitespace=' '), Token(text='Register', whitespace=' '), Token(text='R0', whitespace=' '), Token(text='geladen', whitespace=''), Token(text=',', whitespace=' '), Token(text='für', whitespace=' '), Token(text='return', whitespace=''), Token(text='.', whitespace=' ')], [Token(text='RFRAME', whitespace=''), Token(text=';', whitespace=' '), Token(text=';', whitespace=' '), Token(text='dieser', whitespace=' '), Token(text='Befehl', whitespace=' '), Token(text='korrigiert', whitespace=' '), Token(text='den', whitespace=' '), Token(text='Basepointer', whitespace=' '), Token(text='i6', whitespace=' '), Token(text='und', whitespace=' '), Token(text='Stackpointer', whitespace=' '), Token(text='i7', whitespace=''), Token(text='.', whitespace='')]]\n"
     ]
    }
   ],
   "source": [
    "sentences, labels = train.prepare_tokenized_paragraphs(cache_dir / \"de_data\" / \"tokenized_paragraphs.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(sentences, labels, test_size=0.1, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.011943</td>\n",
       "      <td>0.011528</td>\n",
       "      <td>32:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.008184</td>\n",
       "      <td>0.008171</td>\n",
       "      <td>32:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.007726</td>\n",
       "      <td>0.007424</td>\n",
       "      <td>32:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.006705</td>\n",
       "      <td>0.006933</td>\n",
       "      <td>32:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.006225</td>\n",
       "      <td>0.006536</td>\n",
       "      <td>32:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.006061</td>\n",
       "      <td>0.006045</td>\n",
       "      <td>32:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.005765</td>\n",
       "      <td>0.005611</td>\n",
       "      <td>32:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.005283</td>\n",
       "      <td>0.005218</td>\n",
       "      <td>32:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.004420</td>\n",
       "      <td>0.004872</td>\n",
       "      <td>32:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.004273</td>\n",
       "      <td>0.004806</td>\n",
       "      <td>32:46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "de_model = train.train(x_train, y_train, x_valid, y_valid, n_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(de_model.state_dict(), cache_dir / \"de_data\" / \"model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bminixhofer/miniconda3/lib/python3.7/site-packages/tensorflowjs/converters/keras_h5_conversion.py:122: H5pyDeprecationWarning: The default file mode will change to 'r' (read-only) in h5py 3.0. To suppress this warning, pass the mode you need to h5py.File(), or set the global default h5.get_config().default_file_mode, or set the environment variable H5PY_DEFAULT_READONLY=1. Available modes are: 'r', 'r+', 'w', 'w-'/'x', 'a'. See the docs for details.\n",
      "  return h5py.File(h5file)\n"
     ]
    }
   ],
   "source": [
    "utils.store_model(de_model, \"data/de\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "de_model = models.Network()\n",
    "de_model.load_state_dict(torch.load(cache_dir / \"de_data\" / \"model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89075fa3d5e24b1aac2f0a8aeaf83462",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=586.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Target: Tokenize \n",
      "\n",
      "F1: 0.9991633615400703\n",
      "Precision: 0.9991012479617996\n",
      "Recall: 0.9992254828419558\n",
      "\n",
      "\n",
      "\n",
      "Target: Sentencize \n",
      "\n",
      "F1: 0.9788622524288\n",
      "Precision: 0.9695062375662943\n",
      "Recall: 0.988400603371718\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.evaluate(de_model.cuda().half(), x_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model (english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e047e7ff07d46e992d6de385946f20d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faulty paragraph:\n",
      "[[Token(text='.', whitespace='')], [Token(text='NET', whitespace=' '), Token(text='(', whitespace=''), Token(text='via', whitespace=' '), Token(text='the', whitespace=' '), Token(text='library', whitespace=' '), Token(text=')', whitespace=' '), Token(text='C', whitespace=' '), Token(text='(', whitespace=''), Token(text='via', whitespace=' '), Token(text='the', whitespace=' '), Token(text='library', whitespace=' '), Token(text=')', whitespace=' '), Token(text='C', whitespace=''), Token(text='#', whitespace=' '), Token(text='(', whitespace=''), Token(text='via', whitespace=' '), Token(text='the', whitespace=' '), Token(text='library', whitespace=' '), Token(text=')', whitespace=' '), Token(text='C++', whitespace=' '), Token(text='(', whitespace=''), Token(text='via', whitespace=' '), Token(text='the', whitespace=' '), Token(text='library', whitespace=' '), Token(text='and', whitespace=' '), Token(text=')', whitespace=' '), Token(text='D', whitespace=' '), Token(text='(', whitespace=''), Token(text='via', whitespace=' '), Token(text='the', whitespace=' '), Token(text='library', whitespace=' '), Token(text=')', whitespace=' '), Token(text='Factor', whitespace=' '), Token(text='(', whitespace=''), Token(text='via', whitespace=' '), Token(text='the', whitespace=' '), Token(text='standard', whitespace=' '), Token(text=')', whitespace=' '), Token(text='Java', whitespace=' '), Token(text='(', whitespace=''), Token(text='using', whitespace=' '), Token(text='the', whitespace=' '), Token(text='extension', whitespace=' '), Token(text=')', whitespace=' '), Token(text='Perl', whitespace=' '), Token(text='(', whitespace=''), Token(text='via', whitespace=' '), Token(text='the', whitespace=' '), Token(text='module', whitespace=' '), Token(text=')', whitespace=' '), Token(text='Python', whitespace=' '), Token(text='(', whitespace=''), Token(text='via', whitespace=' '), Token(text=',', whitespace=' '), Token(text=',', whitespace=' '), Token(text=',', whitespace=' '), Token(text=',', whitespace=' '), Token(text='or', whitespace=' '), Token(text=')', whitespace=' '), Token(text='Ruby', whitespace=' '), Token(text='(', whitespace=''), Token(text='via', whitespace=' '), Token(text='the', whitespace=' '), Token(text='library', whitespace=' '), Token(text='and', whitespace=' '), Token(text='and', whitespace=' '), Token(text=')', whitespace=' '), Token(text='Scheme', whitespace=' '), Token(text='(', whitespace=''), Token(text='via', whitespace=' '), Token(text='e.g.', whitespace=' '), Token(text=')', whitespace=' '), Token(text='Racket', whitespace=' '), Token(text='(', whitespace=''), Token(text='via', whitespace=' '), Token(text=')', whitespace=' '), Token(text='Also', whitespace=''), Token(text=',', whitespace=' '), Token(text='multi-parameter', whitespace=' '), Token(text='type', whitespace=' '), Token(text='classes', whitespace=' '), Token(text='in', whitespace=' '), Token(text='Haskell', whitespace=' '), Token(text='and', whitespace=' '), Token(text='Scala', whitespace=' '), Token(text='can', whitespace=' '), Token(text='be', whitespace=' '), Token(text='used', whitespace=' '), Token(text='to', whitespace=' '), Token(text='emulate', whitespace=' '), Token(text='multiple', whitespace=' '), Token(text='dispatch', whitespace=''), Token(text='.', whitespace=' ')], [Token(text='Predicate', whitespace=' '), Token(text='dispatch', whitespace='')]]\n",
      "Faulty paragraph:\n",
      "[[Token(text='.', whitespace='')], [Token(text='MSI', whitespace=''), Token(text=',', whitespace=' '), Token(text='the', whitespace=' '), Token(text='filename', whitespace=' '), Token(text='extension', whitespace=' '), Token(text='of', whitespace=' '), Token(text='Windows', whitespace=' '), Token(text='Installer', whitespace=' '), Token(text='packages', whitespace=' '), Token(text='Medium', whitespace=' '), Token(text='Scale', whitespace=' '), Token(text='Integration', whitespace=''), Token(text=',', whitespace=' '), Token(text='a', whitespace=' '), Token(text='generation', whitespace=' '), Token(text='of', whitespace=' '), Token(text='integrated', whitespace=' '), Token(text='circuit', whitespace=' '), Token(text='chips', whitespace=' '), Token(text='which', whitespace=' '), Token(text='contain', whitespace=' '), Token(text='hundreds', whitespace=' '), Token(text='of', whitespace=' '), Token(text='transistors', whitespace=' '), Token(text='Message', whitespace=' '), Token(text='Signaled', whitespace=' '), Token(text='Interrupts', whitespace=''), Token(text=',', whitespace=' '), Token(text='a', whitespace=' '), Token(text='PCI', whitespace=' '), Token(text='2.2', whitespace=' '), Token(text='interrupt', whitespace=''), Token(text='-', whitespace=''), Token(text='mechanism', whitespace=' '), Token(text='MSI', whitespace=' '), Token(text='protocol', whitespace=''), Token(text=',', whitespace=' '), Token(text='a', whitespace=' '), Token(text='basic', whitespace=' '), Token(text='cache', whitespace=''), Token(text='-', whitespace=''), Token(text='coherence', whitespace=' '), Token(text='protocol', whitespace=' '), Token(text='used', whitespace=' '), Token(text='in', whitespace=' '), Token(text='multiprocessor', whitespace=' '), Token(text='systems', whitespace=' '), Token(text='Micro-Star', whitespace=' '), Token(text='International', whitespace=''), Token(text=',', whitespace=' '), Token(text='a', whitespace=' '), Token(text='Taiwanese', whitespace=' '), Token(text='technology', whitespace=' '), Token(text='corporation', whitespace=' '), Token(text='known', whitespace=' '), Token(text='for', whitespace=' '), Token(text='a', whitespace=' '), Token(text='wide', whitespace=' '), Token(text='range', whitespace=' '), Token(text='of', whitespace=' '), Token(text='hardware', whitespace=' '), Token(text='including', whitespace=' '), Token(text='motherboards', whitespace=' '), Token(text='and', whitespace=' '), Token(text='video', whitespace=' '), Token(text='cards', whitespace=' '), Token(text='Mindless', whitespace=' '), Token(text='Self', whitespace=' '), Token(text='Indulgence', whitespace=''), Token(text=',', whitespace=' '), Token(text='an', whitespace=' '), Token(text='American', whitespace=' '), Token(text='band', whitespace=' '), Token(text='Maison', whitespace=' '), Token(text='du', whitespace=' '), Token(text='Sport', whitespace=' '), Token(text='International', whitespace=''), Token(text=',', whitespace=' '), Token(text='an', whitespace=' '), Token(text='office', whitespace=' '), Token(text='complex', whitespace=' '), Token(text='in', whitespace=' '), Token(text='Lausanne', whitespace=''), Token(text=',', whitespace=' '), Token(text='Switzerland', whitespace=' '), Token(text='Multistakeholder', whitespace=' '), Token(text='initiative', whitespace=''), Token(text=',', whitespace=' '), Token(text='a', whitespace=' '), Token(text='governance', whitespace=' '), Token(text='structure', whitespace=' '), Token(text='Mid-Season', whitespace=' '), Token(text='Invitational', whitespace=''), Token(text=',', whitespace=' '), Token(text='an', whitespace=' '), Token(text='annual', whitespace=' '), Token(text='League', whitespace=' '), Token(text='of', whitespace=' '), Token(text='Legends', whitespace=' '), Token(text='tournament', whitespace=' '), Token(text='Maritime', whitespace=' '), Token(text='safety', whitespace=' '), Token(text='information', whitespace='')]]\n",
      "Faulty paragraph:\n",
      "[[Token(text='.', whitespace='')], [Token(text='NET', whitespace=' '), Token(text='Platform', whitespace=' '), Token(text='Invoke', whitespace=' '), Token(text='(', whitespace=''), Token(text='P', whitespace=''), Token(text='/', whitespace=''), Token(text='Invoke', whitespace=''), Token(text=')', whitespace=' '), Token(text='offers', whitespace=' '), Token(text='the', whitespace=' '), Token(text='same', whitespace=' '), Token(text='ability', whitespace=' '), Token(text='by', whitespace=' '), Token(text='allowing', whitespace=' '), Token(text='calls', whitespace=' '), Token(text='from', whitespace=' '), Token(text='C', whitespace=''), Token(text='#', whitespace=' '), Token(text='to', whitespace=' '), Token(text='what', whitespace=' '), Token(text='Microsoft', whitespace=' '), Token(text='terms', whitespace=' '), Token(text='unmanaged', whitespace=' '), Token(text='code', whitespace=''), Token(text='.', whitespace=' ')], [Token(text='Through', whitespace=' '), Token(text='metadata', whitespace=' '), Token(text='attributes', whitespace=' '), Token(text='the', whitespace=' '), Token(text='programmer', whitespace=' '), Token(text='can', whitespace=' '), Token(text='control', whitespace=' '), Token(text='exactly', whitespace=' '), Token(text='how', whitespace=' '), Token(text='the', whitespace=' '), Token(text='parameters', whitespace=' '), Token(text='and', whitespace=' '), Token(text='results', whitespace=' '), Token(text='are', whitespace=' '), Token(text='marshalled', whitespace=''), Token(text=',', whitespace=' '), Token(text='thus', whitespace=' '), Token(text='avoiding', whitespace=' '), Token(text='the', whitespace=' '), Token(text='external', whitespace=' '), Token(text='glue', whitespace=' '), Token(text='code', whitespace=' '), Token(text='needed', whitespace=' '), Token(text='by', whitespace=' '), Token(text='the', whitespace=' '), Token(text='equivalent', whitespace=' '), Token(text='JNI', whitespace=' '), Token(text='in', whitespace=' '), Token(text='Java', whitespace=''), Token(text='.', whitespace=' ')], [Token(text='P', whitespace=''), Token(text='/', whitespace=''), Token(text='Invoke', whitespace=' '), Token(text='allows', whitespace=' '), Token(text='almost', whitespace=' '), Token(text='complete', whitespace=' '), Token(text='access', whitespace=' '), Token(text='to', whitespace=' '), Token(text='procedural', whitespace=' '), Token(text='APIs', whitespace=' '), Token(text='(', whitespace=''), Token(text='such', whitespace=' '), Token(text='as', whitespace=' '), Token(text='Win32', whitespace=' '), Token(text='or', whitespace=' '), Token(text='POSIX', whitespace=''), Token(text=')', whitespace=''), Token(text=',', whitespace=' '), Token(text='but', whitespace=' '), Token(text='limited', whitespace=' '), Token(text='access', whitespace=' '), Token(text='to', whitespace=' '), Token(text='C++', whitespace=' '), Token(text='class', whitespace=' '), Token(text='libraries', whitespace=''), Token(text='.', whitespace='')]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faulty paragraph:\n",
      "[[Token(text='.', whitespace='')], [Token(text='Burhanpur', whitespace=' '), Token(text='is', whitespace=' '), Token(text='a', whitespace=' '), Token(text='mid-sized', whitespace=' '), Token(text='historical', whitespace=' '), Token(text='city', whitespace=' '), Token(text='in', whitespace=' '), Token(text='the', whitespace=' '), Token(text='Nimar', whitespace=' '), Token(text='region', whitespace=' '), Token(text='of', whitespace=' '), Token(text='Madhya', whitespace=' '), Token(text='Pradesh', whitespace=' '), Token(text='state', whitespace=''), Token(text=',', whitespace=' '), Token(text='India', whitespace=''), Token(text='.', whitespace=' ')], [Token(text='It', whitespace=' '), Token(text='is', whitespace=' '), Token(text='the', whitespace=' '), Token(text='administrative', whitespace=' '), Token(text='seat', whitespace=' '), Token(text='of', whitespace=' '), Token(text='Burhanpur', whitespace=' '), Token(text='District', whitespace=''), Token(text='.', whitespace=' ')], [Token(text='It', whitespace=' '), Token(text='is', whitespace=' '), Token(text='situated', whitespace=' '), Token(text='on', whitespace=' '), Token(text='the', whitespace=' '), Token(text='north', whitespace=' '), Token(text='bank', whitespace=' '), Token(text='of', whitespace=' '), Token(text='the', whitespace=' '), Token(text='Tapti', whitespace=' '), Token(text='River', whitespace=''), Token(text=',', whitespace=' '), Token(text='southwest', whitespace=' '), Token(text='of', whitespace=' '), Token(text='Bhopal', whitespace=' '), Token(text='and', whitespace=' '), Token(text='northeast', whitespace=' '), Token(text='of', whitespace=' '), Token(text='Mumbai', whitespace=''), Token(text='.', whitespace=' ')], [Token(text='The', whitespace=' '), Token(text='city', whitespace=' '), Token(text='has', whitespace=' '), Token(text='a', whitespace=' '), Token(text='Municipal', whitespace=' '), Token(text='Corporation', whitespace=''), Token(text=',', whitespace=' '), Token(text='and', whitespace=' '), Token(text='is', whitespace=' '), Token(text='also', whitespace=' '), Token(text='one', whitespace=' '), Token(text='of', whitespace=' '), Token(text='the', whitespace=' '), Token(text='district', whitespace=' '), Token(text='headquarters', whitespace=' '), Token(text='of', whitespace=' '), Token(text='the', whitespace=' '), Token(text='state', whitespace=' '), Token(text='of', whitespace=' '), Token(text='Madhya', whitespace=' '), Token(text='Pradesh', whitespace=''), Token(text='.', whitespace=' ')], [Token(text='Burhapur', whitespace=' '), Token(text='is', whitespace=' '), Token(text='a', whitespace=' '), Token(text='historical', whitespace=' '), Token(text='city', whitespace=' '), Token(text='that', whitespace=' '), Token(text='is', whitespace=' '), Token(text='well', whitespace=' '), Token(text='connected', whitespace=' '), Token(text='to', whitespace=' '), Token(text='other', whitespace=' '), Token(text='cities', whitespace=' '), Token(text='of', whitespace=' '), Token(text='India', whitespace=' '), Token(text='via', whitespace=' '), Token(text='railway', whitespace=' '), Token(text='network', whitespace=''), Token(text='.', whitespace=' ')], [Token(text='The', whitespace=' '), Token(text='city', whitespace=' '), Token(text='has', whitespace=' '), Token(text='one', whitespace=' '), Token(text='railway', whitespace=' '), Token(text='station', whitespace=''), Token(text=',', whitespace=' '), Token(text='while', whitespace=' '), Token(text='regular', whitespace=' '), Token(text='buses', whitespace=' '), Token(text='are', whitespace=' '), Token(text='available', whitespace=' '), Token(text='for', whitespace=' '), Token(text='travel', whitespace=' '), Token(text='to', whitespace=' '), Token(text='nearby', whitespace=' '), Token(text='cities', whitespace=''), Token(text='.', whitespace=' ')], [Token(text='The', whitespace=' '), Token(text='closest', whitespace=' '), Token(text='airport', whitespace=' '), Token(text='is', whitespace=' '), Token(text='Indore', whitespace=' '), Token(text='Airport', whitespace=''), Token(text=',', whitespace=' '), Token(text='which', whitespace=' '), Token(text='is', whitespace=' '), Token(text='present', whitespace=' '), Token(text='on', whitespace=' '), Token(text='north', whitespace=' '), Token(text='side', whitespace=' '), Token(text='of', whitespace=' '), Token(text='the', whitespace=' '), Token(text='city', whitespace=''), Token(text='.', whitespace=' ')], [Token(text='Within', whitespace=' '), Token(text='the', whitespace=' '), Token(text='city', whitespace=''), Token(text=',', whitespace=' '), Token(text='private', whitespace=' '), Token(text='cars', whitespace=' '), Token(text='and', whitespace=' '), Token(text='cabs', whitespace=' '), Token(text='are', whitespace=' '), Token(text='available', whitespace=' '), Token(text='for', whitespace=' '), Token(text='hire', whitespace=''), Token(text='.', whitespace=' ')], [Token(text='Good', whitespace=' '), Token(text='road', whitespace=' '), Token(text='connectivity', whitespace=' '), Token(text='is', whitespace=' '), Token(text='present', whitespace=''), Token(text=',', whitespace=' '), Token(text='and', whitespace=' '), Token(text='due', whitespace=' '), Token(text='to', whitespace=' '), Token(text='it', whitespace=''), Token(text=',', whitespace=' '), Token(text='goods', whitespace=' '), Token(text='are', whitespace=' '), Token(text='comfortably', whitespace=' '), Token(text='transported', whitespace=' '), Token(text='to', whitespace=' '), Token(text='other', whitespace=' '), Token(text='cities', whitespace=' '), Token(text='via', whitespace=' '), Token(text='truck', whitespace=''), Token(text='.', whitespace='')]]\n"
     ]
    }
   ],
   "source": [
    "sentences, labels = train.prepare_tokenized_paragraphs(cache_dir / \"en_data\" / \"tokenized_paragraphs.pkl\", \"en\", subsample=2_500_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(sentences, labels, test_size=0.1, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.017559</td>\n",
       "      <td>0.016862</td>\n",
       "      <td>28:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.012065</td>\n",
       "      <td>0.012335</td>\n",
       "      <td>28:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.011233</td>\n",
       "      <td>0.011340</td>\n",
       "      <td>28:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.010577</td>\n",
       "      <td>0.010653</td>\n",
       "      <td>28:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.009801</td>\n",
       "      <td>0.010193</td>\n",
       "      <td>28:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.009156</td>\n",
       "      <td>0.009462</td>\n",
       "      <td>28:48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.008747</td>\n",
       "      <td>0.008899</td>\n",
       "      <td>28:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.008379</td>\n",
       "      <td>0.008362</td>\n",
       "      <td>28:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.008053</td>\n",
       "      <td>0.007935</td>\n",
       "      <td>28:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.006977</td>\n",
       "      <td>0.007860</td>\n",
       "      <td>28:50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "en_model = train.train(x_train, y_train, x_valid, y_valid, n_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(en_model.state_dict(), cache_dir / \"en_data\" / \"model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bminixhofer/miniconda3/lib/python3.7/site-packages/tensorflowjs/converters/keras_h5_conversion.py:122: H5pyDeprecationWarning: The default file mode will change to 'r' (read-only) in h5py 3.0. To suppress this warning, pass the mode you need to h5py.File(), or set the global default h5.get_config().default_file_mode, or set the environment variable H5PY_DEFAULT_READONLY=1. Available modes are: 'r', 'r+', 'w', 'w-'/'x', 'a'. See the docs for details.\n",
      "  return h5py.File(h5file)\n"
     ]
    }
   ],
   "source": [
    "utils.store_model(en_model, \"data/en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_model = models.Network()\n",
    "en_model.load_state_dict(torch.load(cache_dir / \"en_data\" / \"model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a09b9c0391b2416082193f45b150a572",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=489.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Target: Tokenize \n",
      "\n",
      "F1: 0.9991483749429025\n",
      "Precision: 0.9991111376555529\n",
      "Recall: 0.9991856150060541\n",
      "\n",
      "\n",
      "\n",
      "Target: Sentencize \n",
      "\n",
      "F1: 0.9639170595081528\n",
      "Precision: 0.9484387271613143\n",
      "Recall: 0.9799089801564257\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.evaluate(en_model.cuda().half(), x_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model = torch.quantization.quantize_dynamic(en_model.float().cpu(), {nn.LSTM, nn.Linear}, dtype=torch.qint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PR curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42a1fa083e9b42278ca034f326db5f5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00efacf24e93422297abdfb419b2a005",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = nnsplit.tokenizer.SoMaJoTokenizer(\"de\")\n",
    "paragraphs = train.xml_to_paragraphs(\"train_data/dewiki-20180920-corpus.xml\", max_n_paragraphs=1000)\n",
    "\n",
    "tokenized_ps = list(tokenizer.split(paragraphs, verbose=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, labels = [], []\n",
    "\n",
    "for p in tokenized_ps:\n",
    "    text, label = utils.label_tokens(p)\n",
    "    \n",
    "    texts.append(text)\n",
    "    labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = utils.load_model(\"data/de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = nnsplit.NNSplit(model, stride=250)\n",
    "\n",
    "preds, all_idx, n_cuts_per_text = splitter._get_raw_preds(texts, batch_size=1024)\n",
    "avg_preds = [x[splitter.start_padding:] for x in splitter._average_preds(texts, preds, all_idx, n_cuts_per_text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bminixhofer/miniconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "scores = np.array([metrics.precision_score(x[:, 1], y[:, 1] > 0.5) for x, y in zip(labels, avg_preds)])\n",
    "scores = scores[scores > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 57, 151, 817, 335, ..., 329, 330, 318, 475])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_labels = np.concatenate(labels)\n",
    "flat_preds = np.concatenate(avg_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, thresholds = metrics.precision_recall_curve(flat_labels[:, 1], flat_preds[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnsplit import NNSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = utils.load_model(\"data/en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[Token(text='Fast', whitespace=''),\n",
       "   Token(text=',', whitespace=' '),\n",
       "   Token(text='robust', whitespace=' '),\n",
       "   Token(text='sentence', whitespace=' '),\n",
       "   Token(text='splitting', whitespace=' '),\n",
       "   Token(text='with', whitespace=' '),\n",
       "   Token(text='Javascript', whitespace=''),\n",
       "   Token(text=',', whitespace=' '),\n",
       "   Token(text='Rust', whitespace=' '),\n",
       "   Token(text='and', whitespace=' '),\n",
       "   Token(text='Python', whitespace=' '),\n",
       "   Token(text='bindings', whitespace=' ')],\n",
       "  [Token(text='Punctuation', whitespace=' '),\n",
       "   Token(text='is', whitespace=' '),\n",
       "   Token(text='not', whitespace=' '),\n",
       "   Token(text='necessary', whitespace=' '),\n",
       "   Token(text='to', whitespace=' '),\n",
       "   Token(text='split', whitespace=' '),\n",
       "   Token(text='sentences', whitespace=' '),\n",
       "   Token(text='correctly', whitespace=' ')],\n",
       "  [Token(text='sometimes', whitespace=' '),\n",
       "   Token(text='even', whitespace=' '),\n",
       "   Token(text='incorrect', whitespace=' '),\n",
       "   Token(text='case', whitespace=' '),\n",
       "   Token(text='is', whitespace=' '),\n",
       "   Token(text='split', whitespace=' '),\n",
       "   Token(text='correctly', whitespace=''),\n",
       "   Token(text='.', whitespace='')]]]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitter = NNSplit(model, threshold=0.1)\n",
    "splitter.split([\"Fast, robust sentence splitting with Javascript, Rust and Python bindings Punctuation is not necessary to split sentences correctly sometimes even incorrect case is split correctly.\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
