{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(\"python_lib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from tqdm.auto import tqdm\n",
    "import nnsplit\n",
    "from nnsplit import train, utils, models, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = Path(\"cache\")\n",
    "cache_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## German"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f65995ecb0014b1aa685963ac7bc1cd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3000000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "paragraphs = train.xml_to_paragraphs(\"train_data/dewiki-20180920-corpus.xml\", max_n_paragraphs=3_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nnsplit.tokenizer.SoMaJoTokenizer(\"de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff8384c996ac41449c9d7c0779a268aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3000000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(cache_dir / \"de_data\" / \"tokenized_paragraphs.pkl\", \"wb\") as f:\n",
    "    for x in tokenizer.split(paragraphs, verbose=True):\n",
    "        f.write(pickle.dumps(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = train.xml_to_paragraphs(\"train_data/enwiki-20181001-corpus.xml\", max_n_paragraphs=3_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nnsplit.tokenizer.SoMaJoTokenizer(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(cache_dir / \"en_data\" / \"tokenized_paragraphs.pkl\", \"wb\") as f:\n",
    "    for x in tokenizer.split(paragraphs, verbose=True):\n",
    "        f.write(pickle.dumps(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model (german)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e214c81f509a42fa9f8e81c26f0896e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentences, labels = train.prepare_tokenized_paragraphs(cache_dir / \"de_data\" / \"tokenized_paragraphs.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(sentences, labels, test_size=0.1, random_state=1234)\n",
    "#x_train, x_valid, y_train, y_valid = torch.load(\"data.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save([x_train, x_valid, y_train, y_valid], \"data.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.011943</td>\n",
       "      <td>0.011528</td>\n",
       "      <td>32:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.008184</td>\n",
       "      <td>0.008171</td>\n",
       "      <td>32:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.007726</td>\n",
       "      <td>0.007424</td>\n",
       "      <td>32:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.006705</td>\n",
       "      <td>0.006933</td>\n",
       "      <td>32:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.006225</td>\n",
       "      <td>0.006536</td>\n",
       "      <td>32:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.006061</td>\n",
       "      <td>0.006045</td>\n",
       "      <td>32:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.005765</td>\n",
       "      <td>0.005611</td>\n",
       "      <td>32:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.005283</td>\n",
       "      <td>0.005218</td>\n",
       "      <td>32:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.004420</td>\n",
       "      <td>0.004872</td>\n",
       "      <td>32:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.004273</td>\n",
       "      <td>0.004806</td>\n",
       "      <td>32:46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "de_model = train.train(x_train, y_train, x_valid, y_valid, n_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(de_model.state_dict(), cache_dir / \"de_data\" / \"model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bminixhofer/miniconda3/lib/python3.7/site-packages/tensorflowjs/converters/keras_h5_conversion.py:122: H5pyDeprecationWarning: The default file mode will change to 'r' (read-only) in h5py 3.0. To suppress this warning, pass the mode you need to h5py.File(), or set the global default h5.get_config().default_file_mode, or set the environment variable H5PY_DEFAULT_READONLY=1. Available modes are: 'r', 'r+', 'w', 'w-'/'x', 'a'. See the docs for details.\n",
      "  return h5py.File(h5file)\n"
     ]
    }
   ],
   "source": [
    "utils.store_model(de_model, \"data/de\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "de_model = models.Network()\n",
    "de_model.load_state_dict(torch.load(cache_dir / \"de_data\" / \"model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=391.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Target: Tokenize \n",
      "\n",
      "F1: 0.9983272884529866\n",
      "Precision: 0.9981672707378787\n",
      "Recall: 0.9984873574816878\n",
      "\n",
      "\n",
      "\n",
      "Target: Sentencize \n",
      "\n",
      "F1: 0.9606947055137844\n",
      "Precision: 0.9401852217352896\n",
      "Recall: 0.9821189441509751\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.evaluate(de_model.cuda().half(), x_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1055e58419ec4a5982786f40b2ab0410",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=391.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Target: Tokenize \n",
      "\n",
      "F1: 0.998631769380241\n"
     ]
    }
   ],
   "source": [
    "train.evaluate(de_model.cuda().half(), x_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model = torch.quantization.quantize_dynamic(de_model.float().cpu(), {nn.LSTM, nn.Linear}, dtype=torch.qint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model (english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fc482c655664a45a10d90a987fb69d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faulty paragraph:\n",
      "[[Token(text='.', whitespace='')], [Token(text='NET', whitespace=' '), Token(text='(', whitespace=''), Token(text='via', whitespace=' '), Token(text='the', whitespace=' '), Token(text='library', whitespace=' '), Token(text=')', whitespace=' '), Token(text='C', whitespace=' '), Token(text='(', whitespace=''), Token(text='via', whitespace=' '), Token(text='the', whitespace=' '), Token(text='library', whitespace=' '), Token(text=')', whitespace=' '), Token(text='C', whitespace=''), Token(text='#', whitespace=' '), Token(text='(', whitespace=''), Token(text='via', whitespace=' '), Token(text='the', whitespace=' '), Token(text='library', whitespace=' '), Token(text=')', whitespace=' '), Token(text='C++', whitespace=' '), Token(text='(', whitespace=''), Token(text='via', whitespace=' '), Token(text='the', whitespace=' '), Token(text='library', whitespace=' '), Token(text='and', whitespace=' '), Token(text=')', whitespace=' '), Token(text='D', whitespace=' '), Token(text='(', whitespace=''), Token(text='via', whitespace=' '), Token(text='the', whitespace=' '), Token(text='library', whitespace=' '), Token(text=')', whitespace=' '), Token(text='Factor', whitespace=' '), Token(text='(', whitespace=''), Token(text='via', whitespace=' '), Token(text='the', whitespace=' '), Token(text='standard', whitespace=' '), Token(text=')', whitespace=' '), Token(text='Java', whitespace=' '), Token(text='(', whitespace=''), Token(text='using', whitespace=' '), Token(text='the', whitespace=' '), Token(text='extension', whitespace=' '), Token(text=')', whitespace=' '), Token(text='Perl', whitespace=' '), Token(text='(', whitespace=''), Token(text='via', whitespace=' '), Token(text='the', whitespace=' '), Token(text='module', whitespace=' '), Token(text=')', whitespace=' '), Token(text='Python', whitespace=' '), Token(text='(', whitespace=''), Token(text='via', whitespace=' '), Token(text=',', whitespace=' '), Token(text=',', whitespace=' '), Token(text=',', whitespace=' '), Token(text=',', whitespace=' '), Token(text='or', whitespace=' '), Token(text=')', whitespace=' '), Token(text='Ruby', whitespace=' '), Token(text='(', whitespace=''), Token(text='via', whitespace=' '), Token(text='the', whitespace=' '), Token(text='library', whitespace=' '), Token(text='and', whitespace=' '), Token(text='and', whitespace=' '), Token(text=')', whitespace=' '), Token(text='Scheme', whitespace=' '), Token(text='(', whitespace=''), Token(text='via', whitespace=' '), Token(text='e.g.', whitespace=' '), Token(text=')', whitespace=' '), Token(text='Racket', whitespace=' '), Token(text='(', whitespace=''), Token(text='via', whitespace=' '), Token(text=')', whitespace=' '), Token(text='Also', whitespace=''), Token(text=',', whitespace=' '), Token(text='multi-parameter', whitespace=' '), Token(text='type', whitespace=' '), Token(text='classes', whitespace=' '), Token(text='in', whitespace=' '), Token(text='Haskell', whitespace=' '), Token(text='and', whitespace=' '), Token(text='Scala', whitespace=' '), Token(text='can', whitespace=' '), Token(text='be', whitespace=' '), Token(text='used', whitespace=' '), Token(text='to', whitespace=' '), Token(text='emulate', whitespace=' '), Token(text='multiple', whitespace=' '), Token(text='dispatch', whitespace=''), Token(text='.', whitespace=' ')], [Token(text='Predicate', whitespace=' '), Token(text='dispatch', whitespace='')]]\n",
      "Faulty paragraph:\n",
      "[[Token(text='.', whitespace='')], [Token(text='CSO', whitespace=''), Token(text=',', whitespace=' '), Token(text='compressed', whitespace=' '), Token(text='file', whitespace=' '), Token(text='format', whitespace=' '), Token(text='for', whitespace=' '), Token(text='ISO', whitespace=' '), Token(text='disc', whitespace=' '), Token(text='images', whitespace=' '), Token(text='Bureau', whitespace=' '), Token(text='of', whitespace=' '), Token(text='Conflict', whitespace=' '), Token(text='and', whitespace=' '), Token(text='Stabilization', whitespace=' '), Token(text='Operations', whitespace=''), Token(text=',', whitespace=' '), Token(text='at', whitespace=' '), Token(text='the', whitespace=' '), Token(text='U.S.', whitespace=' '), Token(text='Department', whitespace=' '), Token(text='of', whitespace=' '), Token(text='State', whitespace=' '), Token(text='Caltech', whitespace=' '), Token(text='Submillimeter', whitespace=' '), Token(text='Observatory', whitespace=''), Token(text=',', whitespace=' '), Token(text='Mauna', whitespace=' '), Token(text='Kea', whitespace=''), Token(text=',', whitespace=' '), Token(text='Hawaii', whitespace=' '), Token(text='Carbonyl', whitespace=' '), Token(text='sulfide', whitespace=' '), Token(text='Colour', whitespace=''), Token(text='-', whitespace=''), Token(text='separation', whitespace=' '), Token(text='overlay', whitespace=' '), Token(text='(', whitespace=''), Token(text='chroma', whitespace=' '), Token(text='key', whitespace=''), Token(text=')', whitespace=' '), Token(text='Combined', whitespace=' '), Token(text='sewer', whitespace=' '), Token(text='overflow', whitespace=' '), Token(text='Composante', whitespace=' '), Token(text='Spatiale', whitespace=' '), Token(text='Optique', whitespace=''), Token(text=',', whitespace=' '), Token(text='a', whitespace=' '), Token(text='French', whitespace=' '), Token(text='spy', whitespace=' '), Token(text='satellite', whitespace=' '), Token(text='Compulsory', whitespace=' '), Token(text='Stock', whitespace=' '), Token(text='Obligation', whitespace=' '), Token(text='Counter-Strike', whitespace=' '), Token(text='Online', whitespace=''), Token(text=',', whitespace=' '), Token(text='a', whitespace=' '), Token(text='2008', whitespace=' '), Token(text='video', whitespace=' '), Token(text='game', whitespace=' '), Token(text='Magdeburg', whitespace=''), Token(text='-', whitespace=''), Token(text='Cochstedt', whitespace=' '), Token(text='Airport', whitespace=' '), Token(text='in', whitespace=' '), Token(text='Saxony', whitespace=' '), Token(text='Anhalt', whitespace=''), Token(text=',', whitespace=' '), Token(text='Germany', whitespace=' '), Token(text='(', whitespace=''), Token(text='IATA', whitespace=' '), Token(text='airport', whitespace=' '), Token(text='code', whitespace=' '), Token(text='CSO', whitespace=''), Token(text=')', whitespace='')]]\n",
      "Faulty paragraph:\n",
      "[[Token(text='.', whitespace='')], [Token(text='Info', whitespace=' '), Token(text='in', whitespace=' '), Token(text='1987', whitespace=' '), Token(text='gave', whitespace=' '), Token(text='the', whitespace=' '), Token(text='Commodore', whitespace=' '), Token(text='64', whitespace=' '), Token(text='version', whitespace=' '), Token(text='four', whitespace=' '), Token(text='stars', whitespace=' '), Token(text='out', whitespace=' '), Token(text='of', whitespace=' '), Token(text='five', whitespace=''), Token(text=',', whitespace=' '), Token(text='describing', whitespace=' '), Token(text='it', whitespace=' '), Token(text='as', whitespace=' '), Token(text='\"', whitespace=''), Token(text='fun', whitespace=' '), Token(text='to', whitespace=' '), Token(text='play', whitespace=''), Token(text=',', whitespace=' '), Token(text='though', whitespace=' '), Token(text='Infocom', whitespace=' '), Token(text='has', whitespace=' '), Token(text='produced', whitespace=' '), Token(text='more', whitespace=' '), Token(text='challenging', whitespace=' '), Token(text='standard', whitespace=''), Token(text='-', whitespace=''), Token(text='level', whitespace=' '), Token(text='text', whitespace=' '), Token(text='adventures', whitespace=' '), Token(text='...', whitespace=' '), Token(text='a', whitespace=' '), Token(text='lot', whitespace=' '), Token(text='of', whitespace=' '), Token(text='giggles', whitespace=' '), Token(text='in', whitespace=' '), Token(text='this', whitespace=' '), Token(text='one', whitespace=''), Token(text='\"', whitespace=''), Token(text='.', whitespace=' ')], [Token(text='The', whitespace=' '), Token(text='magazine', whitespace=' '), Token(text='assured', whitespace=' '), Token(text='readers', whitespace=' '), Token(text='they', whitespace=' '), Token(text='would', whitespace=''), Token(text=\"n't\", whitespace=' '), Token(text='be', whitespace=' '), Token(text='offended', whitespace=' '), Token(text='by', whitespace=' '), Token(text='the', whitespace=' '), Token(text='game', whitespace=''), Token(text=',', whitespace=' '), Token(text='as', whitespace=' '), Token(text='even', whitespace=' '), Token(text='its', whitespace=' '), Token(text='lewdest', whitespace=' '), Token(text='\"', whitespace=''), Token(text='naughtiness', whitespace=''), Token(text='\"', whitespace=' '), Token(text='level', whitespace=' '), Token(text='is', whitespace=' '), Token(text='relatively', whitespace=' '), Token(text='tame', whitespace=''), Token(text='.', whitespace=' ')], [Token(text='In', whitespace=' '), Token(text='1988', whitespace=''), Token(text=',', whitespace=' '), Token(text='Tom', whitespace=' '), Token(text='Clancy', whitespace=' '), Token(text='named', whitespace=' '), Token(text='Leather', whitespace=' '), Token(text='Goddesses', whitespace=' '), Token(text='of', whitespace=' '), Token(text='Phobos', whitespace=' '), Token(text='one', whitespace=' '), Token(text='of', whitespace=' '), Token(text='his', whitespace=' '), Token(text='favorite', whitespace=' '), Token(text='computer', whitespace=' '), Token(text='games', whitespace=''), Token(text=',', whitespace=' '), Token(text='stating', whitespace=' '), Token(text='\"', whitespace=''), Token(text='I', whitespace=''), Token(text=\"'d\", whitespace=' '), Token(text='like', whitespace=' '), Token(text='to', whitespace=' '), Token(text='meet', whitespace=' '), Token(text='whoever', whitespace=' '), Token(text='wrote', whitespace=' '), Token(text='that', whitespace=''), Token(text='.', whitespace=' ')], [Token(text='I', whitespace=' '), Token(text='just', whitespace=' '), Token(text='do', whitespace=''), Token(text=\"n't\", whitespace=' '), Token(text='know', whitespace=' '), Token(text='what', whitespace=' '), Token(text='asylum', whitespace=' '), Token(text='to', whitespace=' '), Token(text='go', whitespace=' '), Token(text='to', whitespace=''), Token(text='\"', whitespace=''), Token(text='.', whitespace='')]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faulty paragraph:\n",
      "[[Token(text='.', whitespace='')], [Token(text='REG', whitespace=' '), Token(text='files', whitespace=' '), Token(text='(', whitespace=''), Token(text='also', whitespace=' '), Token(text='known', whitespace=' '), Token(text='as', whitespace=' '), Token(text='Registration', whitespace=' '), Token(text='entries', whitespace=''), Token(text=')', whitespace=' '), Token(text='are', whitespace=' '), Token(text='text', whitespace=''), Token(text='-', whitespace=''), Token(text='based', whitespace=' '), Token(text='human', whitespace=''), Token(text='-', whitespace=''), Token(text='readable', whitespace=' '), Token(text='files', whitespace=' '), Token(text='for', whitespace=' '), Token(text='exporting', whitespace=' '), Token(text='and', whitespace=' '), Token(text='importing', whitespace=' '), Token(text='portions', whitespace=' '), Token(text='of', whitespace=' '), Token(text='the', whitespace=' '), Token(text='registry', whitespace=''), Token(text='.', whitespace=' ')], [Token(text='On', whitespace=' '), Token(text='Windows', whitespace=' '), Token(text='2000', whitespace=' '), Token(text='and', whitespace=' '), Token(text='later', whitespace=''), Token(text=',', whitespace=' '), Token(text='they', whitespace=' '), Token(text='contain', whitespace=' '), Token(text='the', whitespace=' '), Token(text='string', whitespace=' '), Token(text='Windows', whitespace=' '), Token(text='Registry', whitespace=' '), Token(text='Editor', whitespace=' '), Token(text='Version', whitespace=' '), Token(text='5.00', whitespace=' '), Token(text='at', whitespace=' '), Token(text='the', whitespace=' '), Token(text='beginning', whitespace=' '), Token(text='and', whitespace=' '), Token(text='are', whitespace=' '), Token(text='Unicode', whitespace=''), Token(text='-', whitespace=''), Token(text='based', whitespace=''), Token(text='.', whitespace=' ')], [Token(text='On', whitespace=' '), Token(text='Windows', whitespace=' '), Token(text='9x', whitespace=' '), Token(text='and', whitespace=' '), Token(text='NT', whitespace=' '), Token(text='4.0', whitespace=' '), Token(text='systems', whitespace=''), Token(text=',', whitespace=' '), Token(text='they', whitespace=' '), Token(text='contain', whitespace=' '), Token(text='the', whitespace=' '), Token(text='string', whitespace=' '), Token(text='REGEDIT4', whitespace=' '), Token(text='and', whitespace=' '), Token(text='are', whitespace=' '), Token(text='ANSI', whitespace=''), Token(text='-', whitespace=''), Token(text='based', whitespace=''), Token(text='.', whitespace='')]]\n"
     ]
    }
   ],
   "source": [
    "sentences, labels = train.prepare_tokenized_paragraphs(cache_dir / \"en_data\" / \"tokenized_paragraphs.pkl\", \"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(sentences, labels, test_size=0.1, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.033321</td>\n",
       "      <td>0.034246</td>\n",
       "      <td>23:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.028566</td>\n",
       "      <td>0.028980</td>\n",
       "      <td>23:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.027800</td>\n",
       "      <td>0.027489</td>\n",
       "      <td>20:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.026879</td>\n",
       "      <td>0.027608</td>\n",
       "      <td>19:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.027294</td>\n",
       "      <td>0.027869</td>\n",
       "      <td>19:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.027970</td>\n",
       "      <td>0.027195</td>\n",
       "      <td>21:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.025401</td>\n",
       "      <td>0.026654</td>\n",
       "      <td>19:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.026775</td>\n",
       "      <td>0.026406</td>\n",
       "      <td>19:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.025835</td>\n",
       "      <td>0.025802</td>\n",
       "      <td>19:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.023397</td>\n",
       "      <td>0.025574</td>\n",
       "      <td>19:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.024341</td>\n",
       "      <td>0.024915</td>\n",
       "      <td>19:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.023934</td>\n",
       "      <td>0.024479</td>\n",
       "      <td>21:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.023591</td>\n",
       "      <td>0.023768</td>\n",
       "      <td>21:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.022697</td>\n",
       "      <td>0.023043</td>\n",
       "      <td>21:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.022384</td>\n",
       "      <td>0.022428</td>\n",
       "      <td>20:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.022357</td>\n",
       "      <td>0.021803</td>\n",
       "      <td>19:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.020438</td>\n",
       "      <td>0.021179</td>\n",
       "      <td>19:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.020696</td>\n",
       "      <td>0.020732</td>\n",
       "      <td>19:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.019530</td>\n",
       "      <td>0.020477</td>\n",
       "      <td>19:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.020248</td>\n",
       "      <td>0.020456</td>\n",
       "      <td>19:12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "en_model = train.train(x_train, y_train, x_valid, y_valid, n_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(en_model.state_dict(), cache_dir / \"en_data\" / \"model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bminixhofer/miniconda3/lib/python3.7/site-packages/tensorflowjs/converters/keras_h5_conversion.py:122: H5pyDeprecationWarning: The default file mode will change to 'r' (read-only) in h5py 3.0. To suppress this warning, pass the mode you need to h5py.File(), or set the global default h5.get_config().default_file_mode, or set the environment variable H5PY_DEFAULT_READONLY=1. Available modes are: 'r', 'r+', 'w', 'w-'/'x', 'a'. See the docs for details.\n",
      "  return h5py.File(h5file)\n"
     ]
    }
   ],
   "source": [
    "utils.store_model(en_model, \"data/en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_model = models.Network()\n",
    "en_model.load_state_dict(torch.load(cache_dir / \"en_data\" / \"model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e68bdf2c76242ed8df4705f6d1bcc09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1172.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Target: Tokenize \n",
      "\n",
      "F1: 0.9977046875608959\n",
      "Precision: 0.9981388983517799\n",
      "Recall: 0.9972708543868518\n",
      "\n",
      "\n",
      "\n",
      "Target: Sentencize \n",
      "\n",
      "F1: 0.9406308381250802\n",
      "Precision: 0.9113823973956516\n",
      "Recall: 0.9718188284629058\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.evaluate(en_model.cuda().half(), x_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model = torch.quantization.quantize_dynamic(en_model.float().cpu(), {nn.LSTM, nn.Linear}, dtype=torch.qint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.evaluate(quantized_model, x_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42a1fa083e9b42278ca034f326db5f5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00efacf24e93422297abdfb419b2a005",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = nnsplit.tokenizer.SoMaJoTokenizer(\"de\")\n",
    "paragraphs = train.xml_to_paragraphs(\"train_data/dewiki-20180920-corpus.xml\", max_n_paragraphs=1000)\n",
    "\n",
    "tokenized_ps = list(tokenizer.split(paragraphs, verbose=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, labels = [], []\n",
    "\n",
    "for p in tokenized_ps:\n",
    "    text, label = utils.label_tokens(p)\n",
    "    \n",
    "    texts.append(text)\n",
    "    labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = utils.load_model(\"data/de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = nnsplit.NNSplit(model, stride=250)\n",
    "\n",
    "preds, all_idx, n_cuts_per_text = splitter._get_raw_preds(texts, batch_size=1024)\n",
    "avg_preds = [x[splitter.start_padding:] for x in splitter._average_preds(texts, preds, all_idx, n_cuts_per_text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bminixhofer/miniconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "scores = np.array([metrics.precision_score(x[:, 1], y[:, 1] > 0.5) for x, y in zip(labels, avg_preds)])\n",
    "scores = scores[scores > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 57, 151, 817, 335, ..., 329, 330, 318, 475])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_labels = np.concatenate(labels)\n",
    "flat_preds = np.concatenate(avg_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, thresholds = metrics.precision_recall_curve(flat_labels[:, 1], flat_preds[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnsplit import NNSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = utils.load_model(\"data/de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[Token(text='Ich', whitespace=' '),\n",
       "   Token(text='bin', whitespace=' '),\n",
       "   Token(text='ein', whitespace=' '),\n",
       "   Token(text='Baum', whitespace=' ')],\n",
       "  [Token(text='er', whitespace=' '),\n",
       "   Token(text='ist', whitespace=' '),\n",
       "   Token(text='ein', whitespace=' '),\n",
       "   Token(text='Baum', whitespace=''),\n",
       "   Token(text='.', whitespace='')]]]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitter = NNSplit(model, stride=20, threshold=0.1)\n",
    "splitter.split([\"Ich bin ein Baum er ist ein Baum.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[Token(text='Fast', whitespace=''),\n",
       "   Token(text=',', whitespace=' '),\n",
       "   Token(text='robust', whitespace=' '),\n",
       "   Token(text='sentence', whitespace=' '),\n",
       "   Token(text='splitting', whitespace=' '),\n",
       "   Token(text='with', whitespace=' '),\n",
       "   Token(text='bindings', whitespace=' '),\n",
       "   Token(text='for', whitespace=' '),\n",
       "   Token(text='Python', whitespace=''),\n",
       "   Token(text=',', whitespace=' '),\n",
       "   Token(text='Rust', whitespace=' '),\n",
       "   Token(text='and', whitespace=' '),\n",
       "   Token(text='Javascript', whitespace=' ')],\n",
       "  [Token(text='Punctuation', whitespace=' '),\n",
       "   Token(text='is', whitespace=' '),\n",
       "   Token(text='not', whitespace=' '),\n",
       "   Token(text='necessary', whitespace=' '),\n",
       "   Token(text='to', whitespace=' '),\n",
       "   Token(text='split', whitespace=' '),\n",
       "   Token(text='sentences', whitespace=' '),\n",
       "   Token(text='correctly', whitespace=' ')],\n",
       "  [Token(text='sometimes', whitespace=' '),\n",
       "   Token(text='even', whitespace=' '),\n",
       "   Token(text='incorrect', whitespace=' '),\n",
       "   Token(text='case', whitespace=' '),\n",
       "   Token(text='is', whitespace=' '),\n",
       "   Token(text='split', whitespace=' '),\n",
       "   Token(text='correctly', whitespace=''),\n",
       "   Token(text='.', whitespace='')]]]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitter = NNSplit(model, stride=20, threshold=0.2)\n",
    "splitter.split([\"Fast, robust sentence splitting with bindings for Python, Rust and Javascript Punctuation is not necessary to split sentences correctly sometimes even incorrect case is split correctly.\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
